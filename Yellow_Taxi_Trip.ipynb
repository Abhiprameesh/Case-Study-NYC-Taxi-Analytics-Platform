{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpuGHPCsGjL1tdTdPxQBpT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhiprameesh/Case-Study-NYC-Taxi-Analytics-Platform/blob/main/Yellow_Taxi_Trip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxEflqihe28Q",
        "outputId": "6cf61647-de1b-4e81-bc38-5edfe795c562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to cloud.r-project.org] [Connecting to r2u\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [2 InRelease 47.5 kB/128 kB 37%] [3 InRelease 14.2 kB/129 kB 11%] [Connectin\r                                                                               \rGet:4 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:7 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
            "Get:8 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:9 https://cli.github.com/packages stable/main amd64 Packages [357 B]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,861 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [70.9 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [4,107 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,613 kB]\n",
            "Get:15 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [39.2 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,767 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,302 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,626 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [75.3 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [62.6 kB]\n",
            "Get:21 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [85.2 kB]\n",
            "Get:22 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,785 kB]\n",
            "Get:23 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,913 kB]\n",
            "Fetched 37.7 MB in 5s (7,675 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ],
      "source": [
        "!apt-get update # Update apt-get repository.\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
        "!pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime.\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "!ls\n",
        "\n",
        "# Initialize findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Create a PySpark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\n",
        "    \"/content/C2_2017_Yellow_Taxi_Trip_Data.csv\",\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ")"
      ],
      "metadata": {
        "id": "jJ2_TFQOe62o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnFaQ-y_gdcY",
        "outputId": "b5bb52ef-a86e-4c3f-aeb5-3dde1652d65c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: integer (nullable = true)\n",
            " |-- VendorID: integer (nullable = true)\n",
            " |-- tpep_pickup_datetime: string (nullable = true)\n",
            " |-- tpep_dropoff_datetime: string (nullable = true)\n",
            " |-- passenger_count: integer (nullable = true)\n",
            " |-- trip_distance: double (nullable = true)\n",
            " |-- RatecodeID: integer (nullable = true)\n",
            " |-- store_and_fwd_flag: string (nullable = true)\n",
            " |-- PULocationID: integer (nullable = true)\n",
            " |-- DOLocationID: integer (nullable = true)\n",
            " |-- payment_type: integer (nullable = true)\n",
            " |-- fare_amount: double (nullable = true)\n",
            " |-- extra: double (nullable = true)\n",
            " |-- mta_tax: double (nullable = true)\n",
            " |-- tip_amount: double (nullable = true)\n",
            " |-- tolls_amount: double (nullable = true)\n",
            " |-- improvement_surcharge: double (nullable = true)\n",
            " |-- total_amount: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(5)\n",
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXtocuC_gjm_",
        "outputId": "cf9aee39-56fc-404a-d9a8-f52b47f0e49e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
            "|      _c0|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|\n",
            "+---------+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
            "| 24870114|       2|03/25/2017 8:55:4...| 03/25/2017 9:09:4...|              6|         3.34|         1|                 N|         100|         231|           1|       13.0|  0.0|    0.5|      2.76|         0.0|                  0.3|       16.56|\n",
            "| 35634249|       1|04/11/2017 2:53:2...| 04/11/2017 3:19:5...|              1|          1.8|         1|                 N|         186|          43|           1|       16.0|  0.0|    0.5|       4.0|         0.0|                  0.3|        20.8|\n",
            "|106203690|       1|12/15/2017 7:26:5...| 12/15/2017 7:34:0...|              1|          1.0|         1|                 N|         262|         236|           1|        6.5|  0.0|    0.5|      1.45|         0.0|                  0.3|        8.75|\n",
            "| 38942136|       2|05/07/2017 1:17:5...| 05/07/2017 1:48:1...|              1|          3.7|         1|                 N|         188|          97|           1|       20.5|  0.0|    0.5|      6.39|         0.0|                  0.3|       27.69|\n",
            "| 30841670|       2|04/15/2017 11:32:...| 04/15/2017 11:49:...|              1|         4.37|         1|                 N|           4|         112|           2|       16.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        17.8|\n",
            "+---------+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22699"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe([\"trip_distance\", \"total_amount\"]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LAhg_yWgl--",
        "outputId": "f36e599b-cb7e-49bf-c10e-825c78b0c99a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+------------------+\n",
            "|summary|     trip_distance|      total_amount|\n",
            "+-------+------------------+------------------+\n",
            "|  count|             22699|             22699|\n",
            "|   mean|2.9133129212740796| 16.31050222476416|\n",
            "| stddev|3.6531711828339195|16.097295300636247|\n",
            "|    min|               0.0|            -120.3|\n",
            "|    max|             33.96|           1200.29|\n",
            "+-------+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.summary().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3ONtZh5go01",
        "outputId": "19ab4b79-dd6b-4dc8-b0dc-f802886abcb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+------------------+--------------------+---------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+-------------------+------------------+------------------+---------------------+------------------+\n",
            "|summary|                 _c0|          VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|   passenger_count|     trip_distance|        RatecodeID|store_and_fwd_flag|      PULocationID|      DOLocationID|       payment_type|       fare_amount|              extra|            mta_tax|        tip_amount|      tolls_amount|improvement_surcharge|      total_amount|\n",
            "+-------+--------------------+------------------+--------------------+---------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+-------------------+------------------+------------------+---------------------+------------------+\n",
            "|  count|               22699|             22699|               22699|                22699|             22699|             22699|             22699|             22699|             22699|             22699|              22699|             22699|              22699|              22699|             22699|             22699|                22699|             22699|\n",
            "|   mean| 5.675848617128508E7|1.5562359575311688|                null|                 null|1.6423190448918454|2.9133129212740796|1.0433939821137495|              null|162.41235296709107|161.52799682805409|  1.336887087536896|13.026629366932465|0.33327459359443146| 0.4974448213577691|1.8357813119520605|0.3125415216529417|  0.29955064099751805| 16.31050222476416|\n",
            "| stddev|3.2744929492148545E7|0.4968383961995319|                null|                 null|1.2852311189940233|3.6531711828339195|0.7083908849944119|              null| 66.63337338646542| 70.13969073441459|0.49621105767965806|13.243790516711757|0.46309657874841365|0.03946498733149325|2.8006262722635498| 1.399211931557539| 0.015672737641519604|16.097295300636247|\n",
            "|    min|               12127|                 1|01/01/2017 10:18:...| 01/01/2017 10:02:...|                 0|               0.0|                 1|                 N|                 1|                 1|                  1|            -120.0|               -1.0|               -0.5|               0.0|               0.0|                 -0.3|            -120.3|\n",
            "|    25%|            28511537|                 1|                null|                 null|                 1|              0.99|                 1|              null|               114|               112|                  1|               6.5|                0.0|                0.5|               0.0|               0.0|                  0.3|              8.75|\n",
            "|    50%|            56727323|                 2|                null|                 null|                 1|              1.61|                 1|              null|               162|               162|                  1|               9.5|                0.0|                0.5|              1.35|               0.0|                  0.3|              11.8|\n",
            "|    75%|            85371008|                 2|                null|                 null|                 2|              3.06|                 1|              null|               233|               233|                  2|              14.5|                0.5|                0.5|              2.45|               0.0|                  0.3|              17.8|\n",
            "|    max|           113486300|                 2|12/31/2017 9:57:4...| 12/31/2017 9:55:1...|                 6|             33.96|                99|                 Y|               265|               265|                  4|            999.99|                4.5|                0.5|             200.0|              19.1|                  0.3|           1200.29|\n",
            "+-------+--------------------+------------------+--------------------+---------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+-------------------+------------------+------------------+---------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"passenger_count\").distinct().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lqSfBVXgrx5",
        "outputId": "9d59052c-5097-411f-a414-960903e19eda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+\n",
            "|passenger_count|\n",
            "+---------------+\n",
            "|              1|\n",
            "|              6|\n",
            "|              3|\n",
            "|              5|\n",
            "|              4|\n",
            "|              2|\n",
            "|              0|\n",
            "+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_no_duplicates = df.dropDuplicates()"
      ],
      "metadata": {
        "id": "wm4ERj_Eg88D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropDuplicates([\"tpep_pickup_datetime\", \"PULocationID\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nhvjJN5hFkV",
        "outputId": "db025aba-1b7b-4e70-ca31-3880bcbce0d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[_c0: int, VendorID: int, tpep_pickup_datetime: string, tpep_dropoff_datetime: string, passenger_count: int, trip_distance: double, RatecodeID: int, store_and_fwd_flag: string, PULocationID: int, DOLocationID: int, payment_type: int, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df = df.filter(\n",
        "    (df.trip_distance > 0) &\n",
        "    (df.total_amount > 0)\n",
        ")"
      ],
      "metadata": {
        "id": "l5iR-jJ-hLZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df = df.where(\"trip_distance > 0 AND total_amount > 0\")"
      ],
      "metadata": {
        "id": "Shl053aaiCsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df.filter(clean_df.passenger_count.isNull()).count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKAx4l5piEsb",
        "outputId": "4201fa81-b426-403f-a4ca-349bbe69ecaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df = clean_df.dropna(subset=[\"passenger_count\"])"
      ],
      "metadata": {
        "id": "FAJcxuyyiGRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "clean_df = clean_df.withColumn(\n",
        "    \"revenue_per_km\",\n",
        "    col(\"total_amount\") / (col(\"trip_distance\") * 1.6)\n",
        ")"
      ],
      "metadata": {
        "id": "dYfv70K-iJJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum\n",
        "from pyspark.sql.functions import count\n",
        "from pyspark.sql.functions import avg\n",
        "\n",
        "clean_df.select(sum(\"total_amount\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfUxq95BiOSe",
        "outputId": "64d29d30-c181-4176-e457-3429958b468b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|sum(total_amount)|\n",
            "+-----------------+\n",
            "|366331.9999999227|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df.groupBy(\"passenger_count\") \\\n",
        "    .agg(\n",
        "        sum(\"total_amount\").alias(\"total_revenue\"),\n",
        "        avg(\"trip_distance\").alias(\"avg_distance\"),\n",
        "        count(\"*\").alias(\"trip_count\")\n",
        "    ) \\\n",
        "    .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i7j54_Riznj",
        "outputId": "6be283bd-619a-47af-b79b-7b128cc7d51c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+------------------+------------------+----------+\n",
            "|passenger_count|     total_revenue|      avg_distance|trip_count|\n",
            "+---------------+------------------+------------------+----------+\n",
            "|              1|256639.67999996367| 2.860989306484904|     15991|\n",
            "|              6|11129.749999999973| 3.027267441860466|       688|\n",
            "|              3| 15732.28999999989|3.0660168598524757|       949|\n",
            "|              5|18595.189999999773|2.9472154115586684|      1142|\n",
            "|              4|7421.5000000000355|2.9916666666666654|       450|\n",
            "|              2| 56309.99000000148|3.2209010654490013|      3285|\n",
            "|              0|503.60000000000014|2.6093749999999996|        32|\n",
            "+---------------+------------------+------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import hour, to_timestamp, col\n",
        "\n",
        "# Convert string datetime (with AM/PM) into proper timestamp\n",
        "clean_df = clean_df.withColumn(\n",
        "    \"pickup_time\",\n",
        "    to_timestamp(col(\"tpep_pickup_datetime\"), \"MM/dd/yyyy h:mm:ss a\")\n",
        ")\n",
        "\n",
        "# Extract hour from timestamp (0–23)\n",
        "clean_df = clean_df.withColumn(\n",
        "    \"pickup_hour\",\n",
        "    hour(col(\"pickup_time\"))\n",
        ")\n",
        "\n",
        "# Count number of pickups per hour and sort\n",
        "clean_df.groupBy(\"pickup_hour\") \\\n",
        "    .count() \\\n",
        "    .orderBy(\"pickup_hour\") \\\n",
        "    .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mW1Pvtri7kW",
        "outputId": "9ae4dbb3-d85f-43a3-faa8-002fd0bc22d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----+\n",
            "|pickup_hour|count|\n",
            "+-----------+-----+\n",
            "|          0|  730|\n",
            "|          1|  521|\n",
            "|          2|  382|\n",
            "|          3|  284|\n",
            "|          4|  231|\n",
            "|          5|  234|\n",
            "|          6|  495|\n",
            "|          7|  850|\n",
            "|          8| 1012|\n",
            "|          9| 1102|\n",
            "|         10| 1035|\n",
            "|         11| 1085|\n",
            "|         12| 1106|\n",
            "|         13| 1120|\n",
            "|         14| 1220|\n",
            "|         15| 1163|\n",
            "|         16| 1121|\n",
            "|         17| 1208|\n",
            "|         18| 1440|\n",
            "|         19| 1445|\n",
            "+-----------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required functions\n",
        "from pyspark.sql.functions import hour, to_timestamp, col\n",
        "\n",
        "\n",
        "clean_df = clean_df.withColumn(\n",
        "    \"pickup_time\",\n",
        "    to_timestamp(col(\"tpep_pickup_datetime\"), \"yyyy-MM-dd HH:mm:ss\")\n",
        ")\n",
        "\n",
        "\n",
        "clean_df = clean_df.withColumn(\n",
        "    \"pickup_hour\",\n",
        "    hour(col(\"pickup_time\"))\n",
        ")\n",
        "clean_df.groupBy(\"pickup_hour\") \\\n",
        "    .count() \\\n",
        "    .orderBy(\"pickup_hour\") \\\n",
        "    .show()\n",
        "clean_df.filter(col(\"pickup_time\").isNull()).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ek2-F_xFlVH4",
        "outputId": "98063221-c570-49fa-a069-dc124132bfc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----+\n",
            "|pickup_hour|count|\n",
            "+-----------+-----+\n",
            "|       null|22537|\n",
            "+-----------+-----+\n",
            "\n",
            "+---------+--------+----------------------+----------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+------------------+-----------+-----------+\n",
            "|_c0      |VendorID|tpep_pickup_datetime  |tpep_dropoff_datetime |passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|revenue_per_km    |pickup_time|pickup_hour|\n",
            "+---------+--------+----------------------+----------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+------------------+-----------+-----------+\n",
            "|24870114 |2       |03/25/2017 8:55:43 AM |03/25/2017 9:09:47 AM |6              |3.34         |1         |N                 |100         |231         |1           |13.0       |0.0  |0.5    |2.76      |0.0         |0.3                  |16.56       |3.0988023952095802|null       |null       |\n",
            "|35634249 |1       |04/11/2017 2:53:28 PM |04/11/2017 3:19:58 PM |1              |1.8          |1         |N                 |186         |43          |1           |16.0       |0.0  |0.5    |4.0       |0.0         |0.3                  |20.8        |7.222222222222221 |null       |null       |\n",
            "|106203690|1       |12/15/2017 7:26:56 AM |12/15/2017 7:34:08 AM |1              |1.0          |1         |N                 |262         |236         |1           |6.5        |0.0  |0.5    |1.45      |0.0         |0.3                  |8.75        |5.46875           |null       |null       |\n",
            "|38942136 |2       |05/07/2017 1:17:59 PM |05/07/2017 1:48:14 PM |1              |3.7          |1         |N                 |188         |97          |1           |20.5       |0.0  |0.5    |6.39      |0.0         |0.3                  |27.69       |4.677364864864864 |null       |null       |\n",
            "|30841670 |2       |04/15/2017 11:32:20 PM|04/15/2017 11:49:03 PM|1              |4.37         |1         |N                 |4           |112         |2           |16.5       |0.5  |0.5    |0.0       |0.0         |0.3                  |17.8        |2.545766590389016 |null       |null       |\n",
            "|23345809 |2       |03/25/2017 8:34:11 PM |03/25/2017 8:42:11 PM |6              |2.3          |1         |N                 |161         |236         |1           |9.0        |0.5  |0.5    |2.06      |0.0         |0.3                  |12.36       |3.358695652173913 |null       |null       |\n",
            "|37660487 |2       |05/03/2017 7:04:09 PM |05/03/2017 8:03:47 PM |1              |12.83        |1         |N                 |79          |241         |1           |47.5       |1.0  |0.5    |9.86      |0.0         |0.3                  |59.16       |2.8819173811379573|null       |null       |\n",
            "|69059411 |2       |08/15/2017 5:41:06 PM |08/15/2017 6:03:05 PM |1              |2.98         |1         |N                 |237         |114         |1           |16.0       |1.0  |0.5    |1.78      |0.0         |0.3                  |19.58       |4.106543624161073 |null       |null       |\n",
            "|8433159  |2       |02/04/2017 4:17:07 PM |02/04/2017 4:29:14 PM |1              |1.2          |1         |N                 |234         |249         |2           |9.0        |0.0  |0.5    |0.0       |0.0         |0.3                  |9.8         |5.104166666666667 |null       |null       |\n",
            "|95294817 |1       |11/10/2017 3:20:29 PM |11/10/2017 3:40:55 PM |1              |1.6          |1         |N                 |239         |237         |1           |13.0       |0.0  |0.5    |2.75      |0.0         |0.3                  |16.55       |6.464843749999999 |null       |null       |\n",
            "|18017909 |2       |03/04/2017 11:58:00 AM|03/04/2017 12:13:12 PM|1              |1.77         |1         |N                 |162         |142         |1           |11.5       |0.0  |0.5    |2.46      |0.0         |0.3                  |14.76       |5.21186440677966  |null       |null       |\n",
            "|18600059 |2       |03/05/2017 7:15:30 PM |03/05/2017 7:52:18 PM |2              |18.9         |2         |N                 |236         |132         |1           |52.0       |0.0  |0.5    |14.58     |5.54        |0.3                  |72.92       |2.4113756613756614|null       |null       |\n",
            "|46782248 |1       |06/09/2017 7:00:26 PM |06/09/2017 7:20:11 PM |1              |3.0          |1         |N                 |13          |148         |1           |15.0       |1.0  |0.5    |3.35      |0.0         |0.3                  |20.15       |4.197916666666666 |null       |null       |\n",
            "|94113247 |2       |11/06/2017 11:35:05 PM|11/06/2017 11:42:57 PM|1              |2.39         |1         |N                 |209         |25          |1           |9.5        |0.5  |0.5    |2.16      |0.0         |0.3                  |12.96       |3.3891213389121337|null       |null       |\n",
            "|14168279 |1       |02/22/2017 3:18:31 PM |02/22/2017 3:42:50 PM |1              |3.3          |1         |N                 |238         |161         |1           |17.5       |0.0  |0.5    |4.55      |0.0         |0.3                  |22.85       |4.327651515151516 |null       |null       |\n",
            "|47444401 |2       |06/02/2017 6:41:39 AM |06/02/2017 6:57:47 AM |1              |5.93         |1         |N                 |239         |231         |1           |19.0       |0.0  |0.5    |3.0       |0.0         |0.3                  |22.8        |2.4030354131534573|null       |null       |\n",
            "|69088676 |1       |08/15/2017 7:48:08 PM |08/15/2017 8:00:37 PM |1              |3.6          |1         |N                 |163         |41          |1           |12.5       |1.0  |0.5    |2.85      |0.0         |0.3                  |17.15       |2.977430555555555 |null       |null       |\n",
            "|58691513 |2       |07/10/2017 1:36:31 PM |07/10/2017 1:48:43 PM |2              |1.71         |1         |N                 |142         |100         |1           |9.5        |0.0  |0.5    |0.0       |0.0         |0.3                  |10.3        |3.7646198830409356|null       |null       |\n",
            "|35388828 |2       |04/10/2017 6:12:58 PM |04/10/2017 6:17:39 PM |2              |0.63         |1         |N                 |263         |262         |2           |5.0        |1.0  |0.5    |0.0       |0.0         |0.3                  |6.8         |6.746031746031746 |null       |null       |\n",
            "|18383214 |2       |03/05/2017 4:01:07 AM |03/05/2017 4:14:11 AM |2              |2.77         |1         |N                 |79          |68          |1           |11.5       |0.5  |0.5    |3.2       |0.0         |0.3                  |16.0        |3.6101083032490973|null       |null       |\n",
            "+---------+--------+----------------------+----------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+------------------+-----------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task1\n"
      ],
      "metadata": {
        "id": "lfA7v98fm5PN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-XYUxxMklc1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8327e93e"
      },
      "source": [
        "Task 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a7bf6b1",
        "outputId": "3e234a83-45a0-48de-8988-94b57b41a62e"
      },
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Create the revenue_per_mile column\n",
        "clean_df = clean_df.withColumn(\n",
        "    \"revenue_per_mile\",\n",
        "    col(\"total_amount\") / col(\"trip_distance\")\n",
        ")\n",
        "\n",
        "print(\"Added 'revenue_per_mile' column.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 'revenue_per_mile' column.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f750840",
        "outputId": "ac0b6a4c-46f4-4018-9655-e2d431fa2a4e"
      },
      "source": [
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Compute average revenue per mile\n",
        "avg_revenue_per_mile = clean_df.select(avg(\"revenue_per_mile\").alias(\"average_revenue_per_mile\"))\n",
        "\n",
        "print(\"Average Revenue per Mile:\")\n",
        "avg_revenue_per_mile.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Revenue per Mile:\n",
            "+------------------------+\n",
            "|average_revenue_per_mile|\n",
            "+------------------------+\n",
            "|       9.475535080491763|\n",
            "+------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "e7a6b9ed",
        "outputId": "2d6fcace-ac41-4031-ce18-04c1edb91517"
      },
      "source": [
        "# Top 15 trips with highest revenue per mile\n",
        "print(\"Top 15 Trips with Highest Revenue per Mile:\")\n",
        "clean_df.orderBy(col(\"revenue_per_mile\").desc()).limit(15).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 Trips with Highest Revenue per Mile:\n",
            "+---------+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+------------------+-------------------+-----------+------------------+\n",
            "|      _c0|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|    revenue_per_km|        pickup_time|pickup_hour|  revenue_per_mile|\n",
            "+---------+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+------------------+-------------------+-----------+------------------+\n",
            "|105577859|       2|12/13/2017 12:19:...| 12/13/2017 12:19:...|              1|         0.01|         2|                 N|         132|         132|           1|       52.0|  0.0|    0.5|     17.57|        5.76|                  0.3|       76.13|          4758.125|2017-12-13 12:19:29|         12| 7612.999999999999|\n",
            "| 59839344|       2|07/14/2017 6:09:5...| 07/14/2017 6:11:4...|              1|         0.01|         2|                 N|         132|         132|           2|       52.0|  0.0|    0.5|       0.0|        5.76|                  0.3|       58.56|            3660.0|2017-07-14 06:09:54|          6|            5856.0|\n",
            "| 98165974|       2|11/19/2017 7:17:1...| 11/19/2017 7:17:1...|              1|         0.01|         2|                 N|         264|         239|           2|       52.0|  0.0|    0.5|       0.0|         0.0|                  0.3|        52.8|3299.9999999999995|2017-11-19 07:17:16|          7|            5280.0|\n",
            "| 39498898|       2|05/16/2017 1:33:2...| 05/16/2017 1:33:3...|              1|         0.01|         2|                 N|         100|         100|           2|       52.0|  0.0|    0.5|       0.0|         0.0|                  0.3|        52.8|3299.9999999999995|2017-05-16 13:33:23|         13|            5280.0|\n",
            "|104244836|       2|12/09/2017 11:56:...| 12/09/2017 11:58:...|              1|         0.02|         2|                 N|         186|         186|           1|       52.0|  0.0|    0.5|     11.71|        5.76|                  0.3|       70.27|         2195.9375|2017-12-09 11:56:56|         11|3513.4999999999995|\n",
            "| 68563779|       2|08/13/2017 4:09:3...| 08/13/2017 4:10:5...|              1|         0.03|         3|                 N|         170|         170|           2|       20.5|  0.0|    0.0|       0.0|         0.0|                  0.3|        20.8| 433.3333333333333|2017-08-13 16:09:35|         16| 693.3333333333334|\n",
            "| 90105531|       1|10/25/2017 6:08:1...| 10/25/2017 6:08:3...|              1|          0.1|         2|                 N|         132|         132|           1|       52.0|  4.5|    0.5|       5.0|        5.76|                  0.3|       68.06|425.37499999999994|2017-10-25 18:08:16|         18|             680.6|\n",
            "| 66798269|       1|08/07/2017 10:20:...| 08/07/2017 10:20:...|              1|          0.1|         2|                 N|         162|         163|           1|       52.0|  0.0|    0.5|       8.0|        5.76|                  0.3|       66.56|415.99999999999994|2017-08-07 10:20:05|         10|             665.6|\n",
            "|  2618392|       2|01/10/2017 6:25:4...| 01/10/2017 6:42:0...|              5|         0.02|         1|                 N|         236|         239|           2|       10.5|  1.0|    0.5|       0.0|         0.0|                  0.3|        12.3|           384.375|2017-01-10 18:25:47|         18|             615.0|\n",
            "| 24719314|       2|03/24/2017 8:59:5...| 03/24/2017 9:00:0...|              1|         0.02|         1|                 N|         255|         255|           1|        2.5|  0.5|    0.5|       7.0|         0.0|                  0.3|        10.8|             337.5|2017-03-24 20:59:58|         20|             540.0|\n",
            "| 11157412|       1|02/06/2017 5:50:1...| 02/06/2017 5:51:0...|              1|          2.6|         5|                 N|         226|         226|           1|     999.99|  0.0|    0.0|     200.0|         0.0|                  0.3|     1200.29|         288.53125|2017-02-06 05:50:10|          5|            461.65|\n",
            "| 91660295|       2|10/30/2017 11:23:...| 10/30/2017 11:23:...|              1|         0.32|         5|                 N|         264|          83|           1|      100.0|  0.0|    0.5|      25.2|         0.0|                  0.3|       126.0|         246.09375|2017-10-30 11:23:46|         11|            393.75|\n",
            "| 99495069|       2|11/24/2017 4:32:1...| 11/24/2017 4:32:2...|              1|         0.01|         1|                 N|         263|         263|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         3.8|237.49999999999997|2017-11-24 04:32:18|          4|             380.0|\n",
            "| 67271410|       2|08/08/2017 11:28:...| 08/08/2017 11:29:...|              2|         0.01|         1|                 N|         132|         132|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         3.8|237.49999999999997|2017-08-08 23:28:54|         23|             380.0|\n",
            "| 26279873|       2|03/31/2017 5:29:1...| 03/31/2017 5:29:3...|              1|         0.01|         1|                 N|         249|         249|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         3.8|237.49999999999997|2017-03-31 05:29:19|          5|             380.0|\n",
            "+---------+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+------------------+-------------------+-----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0d5add8"
      },
      "source": [
        "### Task 1: Distance Insights\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2445d63"
      },
      "source": [
        "# Re-creating clean_df as it was not defined in the previous execution context.\n",
        "# Filter out rows with trip_distance or total_amount less than or equal to 0\n",
        "clean_df = df.filter(\n",
        "    (df.trip_distance > 0) &\n",
        "    (df.total_amount > 0)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c031f290"
      },
      "source": [
        "# Drop rows where 'passenger_count' is null\n",
        "clean_df = clean_df.dropna(subset=[\"passenger_count\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf3f68ca"
      },
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Add 'revenue_per_km' column\n",
        "clean_df = clean_df.withColumn(\n",
        "    \"revenue_per_km\",\n",
        "    col(\"total_amount\") / (col(\"trip_distance\") * 1.6)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6b4132d"
      },
      "source": [
        "from pyspark.sql.functions import hour, to_timestamp\n",
        "\n",
        "# Convert string datetime (with AM/PM) into proper timestamp\n",
        "clean_df = clean_df.withColumn(\n",
        "    \"pickup_time\",\n",
        "    to_timestamp(col(\"tpep_pickup_datetime\"), \"MM/dd/yyyy h:mm:ss a\")\n",
        ")\n",
        "\n",
        "# Extract hour from timestamp (0–23)\n",
        "clean_df = clean_df.withColumn(\n",
        "    \"pickup_hour\",\n",
        "    hour(col(\"pickup_time\"))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce242adf",
        "outputId": "c7f8aca5-50bd-44fe-8b75-13f017cbdc22"
      },
      "source": [
        "from pyspark.sql.functions import min, max, avg, percentile_approx\n",
        "\n",
        "# Calculate distance statistics\n",
        "distance_stats = clean_df.select(\n",
        "    min(\"trip_distance\").alias(\"min_trip_distance\"),\n",
        "    max(\"trip_distance\").alias(\"max_trip_distance\"),\n",
        "    avg(\"trip_distance\").alias(\"avg_trip_distance\"),\n",
        "    percentile_approx(\"trip_distance\", 0.5, 100).alias(\"median_trip_distance\")\n",
        ")\n",
        "\n",
        "# Display the results\n",
        "distance_stats.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----------------+------------------+--------------------+\n",
            "|min_trip_distance|max_trip_distance| avg_trip_distance|median_trip_distance|\n",
            "+-----------------+-----------------+------------------+--------------------+\n",
            "|             0.01|            33.96|2.9337808936415817|                 1.6|\n",
            "+-----------------+-----------------+------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05bf2067"
      },
      "source": [
        "### Task 3: Passenger Demand Pattern\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0762a118",
        "outputId": "ab807fb3-6212-4170-f6b6-9d6713d14de1"
      },
      "source": [
        "from pyspark.sql.functions import sum, avg, count\n",
        "\n",
        "# Calculate passenger performance metrics\n",
        "passenger_performance = clean_df.groupBy(\"passenger_count\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"total_trips\"),\n",
        "        avg(\"trip_distance\").alias(\"average_trip_distance\"),\n",
        "        sum(\"total_amount\").alias(\"total_revenue\")\n",
        "    ) \\\n",
        "    .orderBy(\"passenger_count\")\n",
        "\n",
        "# Display the results\n",
        "print(\"Passenger Performance Table:\")\n",
        "passenger_performance.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passenger Performance Table:\n",
            "+---------------+-----------+---------------------+------------------+\n",
            "|passenger_count|total_trips|average_trip_distance|     total_revenue|\n",
            "+---------------+-----------+---------------------+------------------+\n",
            "|              0|         32|   2.6093749999999996|503.60000000000014|\n",
            "|              1|      15991|    2.860989306484904|256639.67999996367|\n",
            "|              2|       3285|   3.2209010654490013| 56309.99000000148|\n",
            "|              3|        949|   3.0660168598524757| 15732.28999999989|\n",
            "|              4|        450|   2.9916666666666654|7421.5000000000355|\n",
            "|              5|       1142|   2.9472154115586684|18595.189999999773|\n",
            "|              6|        688|    3.027267441860466|11129.749999999973|\n",
            "+---------------+-----------+---------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26514ca7"
      },
      "source": [
        "### Task 4: Pickup Time Analytics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c785b9da",
        "outputId": "498c1a56-dd88-456d-e68f-a60c245e0e7c"
      },
      "source": [
        "from pyspark.sql.functions import dayofweek\n",
        "\n",
        "# Extract weekday from timestamp (1 = Sunday, 7 = Saturday)\n",
        "clean_df = clean_df.withColumn(\n",
        "    \"pickup_weekday\",\n",
        "    dayofweek(col(\"pickup_time\"))\n",
        ")\n",
        "\n",
        "print(\"Added 'pickup_weekday' column.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 'pickup_weekday' column.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d387cd7a",
        "outputId": "9e29ad09-72a4-45fe-e9ac-3fce291af89c"
      },
      "source": [
        "from pyspark.sql.functions import count\n",
        "\n",
        "# Count trips per hour\n",
        "trips_per_hour = clean_df.groupBy(\"pickup_hour\") \\\n",
        "    .agg(count(\"*\").alias(\"total_trips\")) \\\n",
        "    .orderBy(\"pickup_hour\")\n",
        "\n",
        "print(\"Trips per Hour:\")\n",
        "trips_per_hour.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trips per Hour:\n",
            "+-----------+-----------+\n",
            "|pickup_hour|total_trips|\n",
            "+-----------+-----------+\n",
            "|          0|        730|\n",
            "|          1|        521|\n",
            "|          2|        382|\n",
            "|          3|        284|\n",
            "|          4|        231|\n",
            "|          5|        234|\n",
            "|          6|        495|\n",
            "|          7|        850|\n",
            "|          8|       1012|\n",
            "|          9|       1102|\n",
            "|         10|       1035|\n",
            "|         11|       1085|\n",
            "|         12|       1106|\n",
            "|         13|       1120|\n",
            "|         14|       1220|\n",
            "|         15|       1163|\n",
            "|         16|       1121|\n",
            "|         17|       1208|\n",
            "|         18|       1440|\n",
            "|         19|       1445|\n",
            "+-----------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e340789",
        "outputId": "69550b2d-b54f-430d-d3e0-2993b178d287"
      },
      "source": [
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Compute average revenue per weekday\n",
        "avg_revenue_per_weekday = clean_df.groupBy(\"pickup_weekday\") \\\n",
        "    .agg(avg(\"total_amount\").alias(\"average_revenue\")) \\\n",
        "    .orderBy(\"pickup_weekday\")\n",
        "\n",
        "print(\"Average Revenue per Weekday:\")\n",
        "avg_revenue_per_weekday.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Revenue per Weekday:\n",
            "+--------------+------------------+\n",
            "|pickup_weekday|   average_revenue|\n",
            "+--------------+------------------+\n",
            "|             1| 16.08965725806481|\n",
            "|             2|16.873152658662416|\n",
            "|             3|16.144403032217614|\n",
            "|             4|16.334314540059673|\n",
            "|             5|16.785422723227946|\n",
            "|             6|16.396767050487565|\n",
            "|             7|15.210205847255702|\n",
            "+--------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oqMNYXj3rL65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1c36b58"
      },
      "source": [
        "### Task 5: Trip Duration Engineering\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "475c5db3",
        "outputId": "7e8b6ea5-8194-4bc4-be8d-f5b46492fa1a"
      },
      "source": [
        "from pyspark.sql.functions import col, to_timestamp\n",
        "\n",
        "# Convert dropoff datetime string to timestamp\n",
        "clean_df = clean_df.withColumn(\n",
        "    \"dropoff_time\",\n",
        "    to_timestamp(col(\"tpep_dropoff_datetime\"), \"MM/dd/yyyy h:mm:ss a\")\n",
        ")\n",
        "\n",
        "# Calculate trip duration in minutes\n",
        "# (dropoff_time - pickup_time) gives an interval, cast to long to get seconds, then divide by 60\n",
        "clean_df = clean_df.withColumn(\n",
        "    \"trip_duration_minutes\",\n",
        "    (col(\"dropoff_time\").cast(\"long\") - col(\"pickup_time\").cast(\"long\")) / 60\n",
        ")\n",
        "\n",
        "print(\"Added 'trip_duration_minutes' column.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 'trip_duration_minutes' column.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5f01b1f",
        "outputId": "075ae289-feed-40c5-ea5c-e6d449351620"
      },
      "source": [
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Compute average duration\n",
        "avg_duration = clean_df.select(avg(\"trip_duration_minutes\").alias(\"average_trip_duration_minutes\"))\n",
        "\n",
        "print(\"Average Trip Duration:\")\n",
        "avg_duration.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Trip Duration:\n",
            "+-----------------------------+\n",
            "|average_trip_duration_minutes|\n",
            "+-----------------------------+\n",
            "|           17.064017689429182|\n",
            "+-----------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "704f4865",
        "outputId": "47a54d99-c31e-4914-ff7e-bd19c864a53f"
      },
      "source": [
        "from pyspark.sql.functions import desc\n",
        "\n",
        "# Longest 10 trips\n",
        "print(\"Top 10 Longest Trips (by duration):\")\n",
        "clean_df.orderBy(desc(\"trip_duration_minutes\")).limit(10).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Longest Trips (by duration):\n",
            "+---------+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+------------------+-------------------+-----------+------------------+--------------+-------------------+---------------------+\n",
            "|      _c0|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|    revenue_per_km|        pickup_time|pickup_hour|  revenue_per_mile|pickup_weekday|       dropoff_time|trip_duration_minutes|\n",
            "+---------+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+------------------+-------------------+-----------+------------------+--------------+-------------------+---------------------+\n",
            "|  9397494|       2|02/09/2017 11:24:...| 02/10/2017 11:24:...|              5|         4.86|         1|                 N|         249|         238|           1|       17.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        18.8|2.4176954732510287|2017-02-09 23:24:58|         23| 3.868312757201646|             5|2017-02-10 23:24:31|              1439.55|\n",
            "| 41838754|       2|05/10/2017 6:53:5...| 05/11/2017 6:53:0...|              5|         0.74|         1|                 N|         161|         162|           2|        7.0|  1.0|    0.5|       0.0|         0.0|                  0.3|         8.8| 7.432432432432433|2017-05-10 18:53:53|         18|11.891891891891893|             4|2017-05-11 18:53:02|              1439.15|\n",
            "| 64882047|       2|07/31/2017 2:04:2...| 08/01/2017 2:03:1...|              5|         0.95|         1|                 N|         162|         161|           1|        8.0|  0.0|    0.5|      1.76|         0.0|                  0.3|       10.56| 6.947368421052632|2017-07-31 14:04:25|         14|11.115789473684211|             2|2017-08-01 14:03:16|              1438.85|\n",
            "| 27567057|       2|04/03/2017 9:02:4...| 04/04/2017 9:01:2...|              1|         1.73|         1|                 N|         107|         249|           2|        9.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        10.3|3.7210982658959537|2017-04-03 21:02:49|         21|5.9537572254335265|             2|2017-04-04 21:01:28|              1438.65|\n",
            "| 30152527|       2|04/13/2017 11:41:...| 04/14/2017 11:39:...|              2|         1.99|         1|                 N|         239|         166|           2|        7.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         8.3|  2.60678391959799|2017-04-13 23:41:09|         23| 4.170854271356784|             5|2017-04-14 23:39:42|              1438.55|\n",
            "| 55106408|       2|06/30/2017 8:36:0...| 07/01/2017 8:34:2...|              1|         1.09|         1|                 N|          48|         161|           2|        7.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         8.3| 4.759174311926605|2017-06-30 20:36:00|         20| 7.614678899082569|             6|2017-07-01 20:34:28|   1438.4666666666667|\n",
            "|106024452|       2|12/14/2017 5:21:3...| 12/15/2017 5:19:5...|              1|         3.13|         1|                 N|         236|         186|           2|       29.5|  1.0|    0.5|       0.0|         0.0|                  0.3|        31.3|              6.25|2017-12-14 17:21:37|         17|              10.0|             5|2017-12-15 17:19:53|   1438.2666666666667|\n",
            "| 50245266|       2|06/14/2017 11:51:...| 06/15/2017 11:49:...|              5|         2.93|         1|                 N|         107|         141|           2|       18.5|  0.0|    0.5|       0.0|         0.0|                  0.3|        19.3|4.1168941979522184|2017-06-14 11:51:18|         11| 6.587030716723549|             4|2017-06-15 11:49:20|   1438.0333333333333|\n",
            "| 54862162|       2|06/27/2017 4:52:0...| 06/28/2017 4:49:5...|              1|         15.6|         2|                 N|         163|         132|           1|       52.0|  4.5|    0.5|       0.0|         0.0|                  0.3|        57.3|2.2956730769230766|2017-06-27 16:52:07|         16| 3.673076923076923|             3|2017-06-28 16:49:57|   1437.8333333333333|\n",
            "| 83629358|       2|10/05/2017 10:51:...| 10/06/2017 10:48:...|              1|          4.7|         1|                 N|         211|         142|           2|       23.5|  0.0|    0.5|       0.0|         0.0|                  0.3|        24.3| 3.231382978723404|2017-10-05 10:51:03|         10| 5.170212765957447|             5|2017-10-06 10:48:01|   1436.9666666666667|\n",
            "+---------+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+------------------+-------------------+-----------+------------------+--------------+-------------------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5b8341e",
        "outputId": "9f7830e0-bc54-4dd4-8eaa-ddf38cd9468e"
      },
      "source": [
        "from pyspark.sql.functions import count\n",
        "\n",
        "# Trips shorter than 2 minutes (data anomaly check)\n",
        "short_trips_count = clean_df.filter(col(\"trip_duration_minutes\") < 2).count()\n",
        "\n",
        "print(f\"Number of trips shorter than 2 minutes: {short_trips_count}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trips shorter than 2 minutes: 383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea853647",
        "outputId": "ce6ff59b-b091-4337-deb2-e66c45a7c51d"
      },
      "source": [
        "import os\n",
        "if not os.path.exists(\"/content/taxi+_zone_lookup.csv\"):\n",
        "    !wget -q https://raw.githubusercontent.com/databricks-academy/data-engineering-with-databricks-2023/main/retail-org/taxi_zone_lookup.csv -O /content/taxi+_zone_lookup.csv\n",
        "zone_df = spark.read.csv(\n",
        "    \"/content/taxi+_zone_lookup.csv\",\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ")\n",
        "print(\"Zone lookup data loaded into zone_df.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zone lookup data loaded into zone_df.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e16c6f8b",
        "outputId": "66b0e125-b76c-46ae-fb52-9ac909891357"
      },
      "source": [
        "from pyspark.sql.functions import col, count, lit\n",
        "\n",
        "# Count total trips to calculate percentages later\n",
        "total_trips = clean_df_with_borough.count()\n",
        "\n",
        "# Group by 'Borough', count trips, and calculate the proportion\n",
        "borough_trip_proportions = clean_df_with_borough.groupBy(\"Borough\") \\\n",
        "    .agg(count(\"*\").alias(\"trip_count\")) \\\n",
        "    .withColumn(\"proportion\", col(\"trip_count\") / lit(total_trips) * 100) \\\n",
        "    .orderBy(col(\"trip_count\").desc())\n",
        "\n",
        "print(\"Proportion of Trips per Borough:\")\n",
        "borough_trip_proportions.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proportion of Trips per Borough:\n",
            "+-------+----------+----------+\n",
            "|Borough|trip_count|proportion|\n",
            "+-------+----------+----------+\n",
            "|null   |22537     |100.0     |\n",
            "+-------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ad70dd6",
        "outputId": "02461fde-fceb-418a-aee7-fcb75be61974"
      },
      "source": [
        "print(\"Schema of zone_df:\")\n",
        "zone_df.printSchema()\n",
        "\n",
        "print(\"First 5 rows of zone_df:\")\n",
        "zone_df.show(5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema of zone_df:\n",
            "root\n",
            " |-- LocationID: integer (nullable = true)\n",
            " |-- Borough: string (nullable = true)\n",
            " |-- Zone: string (nullable = true)\n",
            " |-- service_zone: string (nullable = true)\n",
            "\n",
            "First 5 rows of zone_df:\n",
            "+----------+-------+----+------------+\n",
            "|LocationID|Borough|Zone|service_zone|\n",
            "+----------+-------+----+------------+\n",
            "+----------+-------+----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e61a70b3",
        "outputId": "fe8a8835-de93-4118-ebbb-8c1e79533dc2"
      },
      "source": [
        "print(f\"Count of rows in zone_df: {zone_df.count()}\")\n",
        "\n",
        "print(\"Content of taxi_zone_lookup_renamed.csv:\")\n",
        "!head /content/taxi_zone_lookup_renamed.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of rows in zone_df: 0\n",
            "Content of taxi_zone_lookup_renamed.csv:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "416a2d88",
        "outputId": "8da89d10-128d-455d-8531-c2c99440d792"
      },
      "source": [
        "import os\n",
        "\n",
        "original_file_path = \"/content/taxi+_zone_lookup.csv\"\n",
        "renamed_file_path = \"/content/taxi_zone_lookup_renamed.csv\"\n",
        "\n",
        "# Remove the renamed file if it exists and is empty/corrupted\n",
        "if os.path.exists(renamed_file_path):\n",
        "    os.remove(renamed_file_path)\n",
        "    print(f\"Removed existing file: {renamed_file_path}\")\n",
        "\n",
        "# Re-download the file\n",
        "!wget -q https://raw.githubusercontent.com/databricks-academy/data-engineering-with-databricks-2023/main/retail-org/taxi_zone_lookup.csv -O \"$original_file_path\"\n",
        "print(f\"Re-downloaded file to: {original_file_path}\")\n",
        "\n",
        "# Rename the file again\n",
        "if os.path.exists(original_file_path) and not os.path.exists(renamed_file_path):\n",
        "    !mv \"$original_file_path\" \"$renamed_file_path\"\n",
        "    print(f\"File renamed from {original_file_path} to {renamed_file_path}\")\n",
        "\n",
        "print(\"File download and renaming complete.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed existing file: /content/taxi_zone_lookup_renamed.csv\n",
            "Re-downloaded file to: /content/taxi+_zone_lookup.csv\n",
            "File renamed from /content/taxi+_zone_lookup.csv to /content/taxi_zone_lookup_renamed.csv\n",
            "File download and renaming complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9bccc3b",
        "outputId": "adf9b056-e85a-4fd6-c41c-a6330da9b452"
      },
      "source": [
        "import os\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "# Define the original and new file paths\n",
        "original_file_path = \"/content/taxi+_zone_lookup.csv\"\n",
        "renamed_file_path = \"/content/taxi_zone_lookup_renamed.csv\"\n",
        "\n",
        "# Define the schema explicitly for the zone lookup file\n",
        "zone_schema = StructType([\n",
        "    StructField(\"LocationID\", IntegerType(), True),\n",
        "    StructField(\"Borough\", StringType(), True),\n",
        "    StructField(\"Zone\", StringType(), True),\n",
        "    StructField(\"service_zone\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Load zone_df using the renamed file and explicit schema\n",
        "zone_df = spark.read.csv(\n",
        "    renamed_file_path,\n",
        "    header=True,\n",
        "    schema=zone_schema # Use explicit schema instead of inferSchema=True\n",
        ")\n",
        "\n",
        "# Join clean_df with zone_df on PULocationID\n",
        "clean_df_with_borough = clean_df.join(\n",
        "    zone_df.withColumnRenamed(\"LocationID\", \"PULocationID\"),\n",
        "    on=\"PULocationID\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "print(\"Joined taxi trip data with zone lookup data to add borough information.\")\n",
        "clean_df_with_borough.printSchema()\n",
        "clean_df_with_borough.show(5, truncate=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joined taxi trip data with zone lookup data to add borough information.\n",
            "root\n",
            " |-- PULocationID: integer (nullable = true)\n",
            " |-- _c0: integer (nullable = true)\n",
            " |-- VendorID: integer (nullable = true)\n",
            " |-- tpep_pickup_datetime: string (nullable = true)\n",
            " |-- tpep_dropoff_datetime: string (nullable = true)\n",
            " |-- passenger_count: integer (nullable = true)\n",
            " |-- trip_distance: double (nullable = true)\n",
            " |-- RatecodeID: integer (nullable = true)\n",
            " |-- store_and_fwd_flag: string (nullable = true)\n",
            " |-- DOLocationID: integer (nullable = true)\n",
            " |-- payment_type: integer (nullable = true)\n",
            " |-- fare_amount: double (nullable = true)\n",
            " |-- extra: double (nullable = true)\n",
            " |-- mta_tax: double (nullable = true)\n",
            " |-- tip_amount: double (nullable = true)\n",
            " |-- tolls_amount: double (nullable = true)\n",
            " |-- improvement_surcharge: double (nullable = true)\n",
            " |-- total_amount: double (nullable = true)\n",
            " |-- revenue_per_km: double (nullable = true)\n",
            " |-- pickup_time: timestamp (nullable = true)\n",
            " |-- pickup_hour: integer (nullable = true)\n",
            " |-- revenue_per_mile: double (nullable = true)\n",
            " |-- pickup_weekday: integer (nullable = true)\n",
            " |-- dropoff_time: timestamp (nullable = true)\n",
            " |-- trip_duration_minutes: double (nullable = true)\n",
            " |-- Borough: string (nullable = true)\n",
            " |-- Zone: string (nullable = true)\n",
            " |-- service_zone: string (nullable = true)\n",
            "\n",
            "+------------+---------+--------+----------------------+----------------------+---------------+-------------+----------+------------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+------------------+-------------------+-----------+------------------+--------------+-------------------+---------------------+-------+----+------------+\n",
            "|PULocationID|_c0      |VendorID|tpep_pickup_datetime  |tpep_dropoff_datetime |passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|revenue_per_km    |pickup_time        |pickup_hour|revenue_per_mile  |pickup_weekday|dropoff_time       |trip_duration_minutes|Borough|Zone|service_zone|\n",
            "+------------+---------+--------+----------------------+----------------------+---------------+-------------+----------+------------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+------------------+-------------------+-----------+------------------+--------------+-------------------+---------------------+-------+----+------------+\n",
            "|100         |24870114 |2       |03/25/2017 8:55:43 AM |03/25/2017 9:09:47 AM |6              |3.34         |1         |N                 |231         |1           |13.0       |0.0  |0.5    |2.76      |0.0         |0.3                  |16.56       |3.0988023952095802|2017-03-25 08:55:43|8          |4.958083832335329 |7             |2017-03-25 09:09:47|14.066666666666666   |null   |null|null        |\n",
            "|186         |35634249 |1       |04/11/2017 2:53:28 PM |04/11/2017 3:19:58 PM |1              |1.8          |1         |N                 |43          |1           |16.0       |0.0  |0.5    |4.0       |0.0         |0.3                  |20.8        |7.222222222222221 |2017-04-11 14:53:28|14         |11.555555555555555|3             |2017-04-11 15:19:58|26.5                 |null   |null|null        |\n",
            "|262         |106203690|1       |12/15/2017 7:26:56 AM |12/15/2017 7:34:08 AM |1              |1.0          |1         |N                 |236         |1           |6.5        |0.0  |0.5    |1.45      |0.0         |0.3                  |8.75        |5.46875           |2017-12-15 07:26:56|7          |8.75              |6             |2017-12-15 07:34:08|7.2                  |null   |null|null        |\n",
            "|188         |38942136 |2       |05/07/2017 1:17:59 PM |05/07/2017 1:48:14 PM |1              |3.7          |1         |N                 |97          |1           |20.5       |0.0  |0.5    |6.39      |0.0         |0.3                  |27.69       |4.677364864864864 |2017-05-07 13:17:59|13         |7.4837837837837835|1             |2017-05-07 13:48:14|30.25                |null   |null|null        |\n",
            "|4           |30841670 |2       |04/15/2017 11:32:20 PM|04/15/2017 11:49:03 PM|1              |4.37         |1         |N                 |112         |2           |16.5       |0.5  |0.5    |0.0       |0.0         |0.3                  |17.8        |2.545766590389016 |2017-04-15 23:32:20|23         |4.073226544622425 |7             |2017-04-15 23:49:03|16.716666666666665   |null   |null|null        |\n",
            "+------------+---------+--------+----------------------+----------------------+---------------+-------------+----------+------------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+------------------+-------------------+-----------+------------------+--------------+-------------------+---------------------+-------+----+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3009db69",
        "outputId": "ce193279-aefb-4f64-a8ef-e2c7981ed57d"
      },
      "source": [
        "print(\"Schema of zone_df after loading with explicit schema:\")\n",
        "zone_df.printSchema()\n",
        "\n",
        "print(\"First 5 rows of zone_df after loading with explicit schema:\")\n",
        "zone_df.show(5, truncate=False)\n",
        "\n",
        "print(f\"Count of rows in zone_df: {zone_df.count()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema of zone_df after loading with explicit schema:\n",
            "root\n",
            " |-- LocationID: integer (nullable = true)\n",
            " |-- Borough: string (nullable = true)\n",
            " |-- Zone: string (nullable = true)\n",
            " |-- service_zone: string (nullable = true)\n",
            "\n",
            "First 5 rows of zone_df after loading with explicit schema:\n",
            "+----------+-------+----+------------+\n",
            "|LocationID|Borough|Zone|service_zone|\n",
            "+----------+-------+----+------------+\n",
            "+----------+-------+----+------------+\n",
            "\n",
            "Count of rows in zone_df: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b69ee3ff",
        "outputId": "56783919-ddde-48db-c7ec-2c8ac7007e94"
      },
      "source": [
        "print(\"Verifying content of taxi_zone_lookup_renamed.csv:\")\n",
        "!ls -lh /content/taxi_zone_lookup_renamed.csv\n",
        "!head /content/taxi_zone_lookup_renamed.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verifying content of taxi_zone_lookup_renamed.csv:\n",
            "-rw-r--r-- 1 root root 0 Feb 26 06:46 /content/taxi_zone_lookup_renamed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b9a8e7d",
        "outputId": "89ee96ac-7477-42fd-80c0-f4e9f850be00"
      },
      "source": [
        "import os\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "original_file_path = \"/content/taxi+_zone_lookup.csv\"\n",
        "renamed_file_path = \"/content/taxi_zone_lookup_renamed.csv\"\n",
        "\n",
        "# Ensure the old, empty file is removed before trying to download again\n",
        "if os.path.exists(renamed_file_path) and os.path.getsize(renamed_file_path) == 0:\n",
        "    os.remove(renamed_file_path)\n",
        "    print(f\"Removed empty file: {renamed_file_path}\")\n",
        "\n",
        "# Re-download the file. Removed -q to potentially see errors.\n",
        "# Remove the old original file if it exists to ensure a fresh download\n",
        "if os.path.exists(original_file_path):\n",
        "    os.remove(original_file_path)\n",
        "!wget https://raw.githubusercontent.com/databricks-academy/data-engineering-with-databricks-2023/main/retail-org/taxi_zone_lookup.csv -O \"$original_file_path\"\n",
        "print(f\"Re-downloaded file to: {original_file_path}\")\n",
        "\n",
        "# Rename the file again\n",
        "if os.path.exists(original_file_path):\n",
        "    if os.path.exists(renamed_file_path):\n",
        "        os.remove(renamed_file_path) # Remove if already exists to prevent mv error\n",
        "    !mv \"$original_file_path\" \"$renamed_file_path\"\n",
        "    print(f\"File renamed from {original_file_path} to {renamed_file_path}\")\n",
        "\n",
        "# Verify if the downloaded file is not empty\n",
        "if not os.path.exists(renamed_file_path) or os.path.getsize(renamed_file_path) == 0:\n",
        "    print(f\"Error: The downloaded file {renamed_file_path} is still empty. Cannot proceed with loading data.\")\n",
        "    # The subtask cannot be completed successfully without the data\n",
        "else:\n",
        "    print(f\"File {renamed_file_path} successfully downloaded and verified (size: {os.path.getsize(renamed_file_path)} bytes).\")\n",
        "\n",
        "    # Define the schema explicitly for the zone lookup file\n",
        "    zone_schema = StructType([\n",
        "        StructField(\"LocationID\", IntegerType(), True),\n",
        "        StructField(\"Borough\", StringType(), True),\n",
        "        StructField(\"Zone\", StringType(), True),\n",
        "        StructField(\"service_zone\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "    # Load zone_df using the renamed file and explicit schema\n",
        "    zone_df = spark.read.csv(\n",
        "        renamed_file_path,\n",
        "        header=True,\n",
        "        schema=zone_schema # Use explicit schema instead of inferSchema=True\n",
        "    )\n",
        "\n",
        "    # Join clean_df with zone_df on PULocationID\n",
        "    clean_df_with_borough = clean_df.join(\n",
        "        zone_df.withColumnRenamed(\"LocationID\", \"PULocationID\"),\n",
        "        on=\"PULocationID\",\n",
        "        how=\"left\"\n",
        "    )\n",
        "\n",
        "    print(\"Joined taxi trip data with zone lookup data to add borough information.\")\n",
        "    clean_df_with_borough.printSchema()\n",
        "    clean_df_with_borough.show(5, truncate=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed empty file: /content/taxi_zone_lookup_renamed.csv\n",
            "--2026-02-26 06:47:45--  https://raw.githubusercontent.com/databricks-academy/data-engineering-with-databricks-2023/main/retail-org/taxi_zone_lookup.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2026-02-26 06:47:45 ERROR 404: Not Found.\n",
            "\n",
            "Re-downloaded file to: /content/taxi+_zone_lookup.csv\n",
            "File renamed from /content/taxi+_zone_lookup.csv to /content/taxi_zone_lookup_renamed.csv\n",
            "Error: The downloaded file /content/taxi_zone_lookup_renamed.csv is still empty. Cannot proceed with loading data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0ebdead",
        "outputId": "c5ba3d88-3b06-412d-fe1b-b21d4a625f90"
      },
      "source": [
        "import os\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "original_download_path = \"/content/taxi_zone_lookup.csv\"\n",
        "renamed_file_path = \"/content/taxi_zone_lookup_renamed.csv\"\n",
        "\n",
        "# 1. Remove any existing files to ensure a clean download\n",
        "if os.path.exists(original_download_path):\n",
        "    os.remove(original_download_path)\n",
        "    print(f\"Removed existing file: {original_download_path}\")\n",
        "if os.path.exists(renamed_file_path):\n",
        "    os.remove(renamed_file_path)\n",
        "    print(f\"Removed existing file: {renamed_file_path}\")\n",
        "\n",
        "# 2. Download the file from the corrected URL\n",
        "new_url = \"https://raw.githubusercontent.com/toddwschneider/nyc-taxi-data/master/taxi_zone_lookup.csv\"\n",
        "!wget \"{new_url}\" -O \"{original_download_path}\"\n",
        "print(f\"Downloaded file from {new_url} to: {original_download_path}\")\n",
        "\n",
        "# 3. Rename the downloaded file\n",
        "if os.path.exists(original_download_path):\n",
        "    os.rename(original_download_path, renamed_file_path)\n",
        "    print(f\"File renamed from {original_download_path} to {renamed_file_path}\")\n",
        "else:\n",
        "    print(f\"Error: Original downloaded file {original_download_path} not found.\")\n",
        "\n",
        "# 4. Verify that the file exists and is not empty\n",
        "if not os.path.exists(renamed_file_path) or os.path.getsize(renamed_file_path) == 0:\n",
        "    print(f\"Error: The downloaded file {renamed_file_path} is empty or does not exist. Cannot proceed with loading data.\")\n",
        "else:\n",
        "    print(f\"File {renamed_file_path} successfully downloaded and verified (size: {os.path.getsize(renamed_file_path)} bytes).\\n\")\n",
        "\n",
        "    # 5. Define the zone_schema explicitly\n",
        "    zone_schema = StructType([\n",
        "        StructField(\"LocationID\", IntegerType(), True),\n",
        "        StructField(\"Borough\", StringType(), True),\n",
        "        StructField(\"Zone\", StringType(), True),\n",
        "        StructField(\"service_zone\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "    # 6. Load zone_df using the renamed file and explicit schema\n",
        "    zone_df = spark.read.csv(\n",
        "        renamed_file_path,\n",
        "        header=True,\n",
        "        schema=zone_schema\n",
        "    )\n",
        "\n",
        "    # 7. Print the schema of zone_df and show its first 5 rows\n",
        "    print(\"Schema of zone_df:\")\n",
        "    zone_df.printSchema()\n",
        "    print(\"\\nFirst 5 rows of zone_df:\")\n",
        "    zone_df.show(5, truncate=False)\n",
        "\n",
        "    # 8. Print the total count of rows in zone_df\n",
        "    print(f\"\\nTotal count of rows in zone_df: {zone_df.count()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed existing file: /content/taxi_zone_lookup_renamed.csv\n",
            "--2026-02-26 06:48:28--  https://raw.githubusercontent.com/toddwschneider/nyc-taxi-data/master/taxi_zone_lookup.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2026-02-26 06:48:28 ERROR 404: Not Found.\n",
            "\n",
            "Downloaded file from https://raw.githubusercontent.com/toddwschneider/nyc-taxi-data/master/taxi_zone_lookup.csv to: /content/taxi_zone_lookup.csv\n",
            "File renamed from /content/taxi_zone_lookup.csv to /content/taxi_zone_lookup_renamed.csv\n",
            "Error: The downloaded file /content/taxi_zone_lookup_renamed.csv is empty or does not exist. Cannot proceed with loading data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82d9e025",
        "outputId": "9e8fe50a-d1bc-4f81-bf84-a25bfa4d4942"
      },
      "source": [
        "import os\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "original_download_path = \"/content/taxi_zone_lookup.csv\"\n",
        "renamed_file_path = \"/content/taxi_zone_lookup_renamed.csv\"\n",
        "\n",
        "# 1. Remove any existing files to ensure a clean download\n",
        "if os.path.exists(original_download_path):\n",
        "    os.remove(original_download_path)\n",
        "    print(f\"Removed existing file: {original_download_path}\")\n",
        "if os.path.exists(renamed_file_path):\n",
        "    os.remove(renamed_file_path)\n",
        "    print(f\"Removed existing file: {renamed_file_path}\")\n",
        "\n",
        "# 2. Download the file from the corrected URL\n",
        "# Found a working URL from a similar dataset source\n",
        "new_url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\"\n",
        "!wget \"{new_url}\" -O \"{original_download_path}\"\n",
        "print(f\"Downloaded file from {new_url} to: {original_download_path}\")\n",
        "\n",
        "# 3. Rename the downloaded file\n",
        "if os.path.exists(original_download_path):\n",
        "    os.rename(original_download_path, renamed_file_path)\n",
        "    print(f\"File renamed from {original_download_path} to {renamed_file_path}\")\n",
        "else:\n",
        "    print(f\"Error: Original downloaded file {original_download_path} not found.\")\n",
        "\n",
        "# 4. Verify that the file exists and is not empty\n",
        "if not os.path.exists(renamed_file_path) or os.path.getsize(renamed_file_path) == 0:\n",
        "    print(f\"Error: The downloaded file {renamed_file_path} is empty or does not exist. Cannot proceed with loading data.\")\n",
        "else:\n",
        "    print(f\"File {renamed_file_path} successfully downloaded and verified (size: {os.path.getsize(renamed_file_path)} bytes).\\n\")\n",
        "\n",
        "    # 5. Define the zone_schema explicitly\n",
        "    zone_schema = StructType([\n",
        "        StructField(\"LocationID\", IntegerType(), True),\n",
        "        StructField(\"Borough\", StringType(), True),\n",
        "        StructField(\"Zone\", StringType(), True),\n",
        "        StructField(\"service_zone\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "    # 6. Load zone_df using the renamed file and explicit schema\n",
        "    zone_df = spark.read.csv(\n",
        "        renamed_file_path,\n",
        "        header=True,\n",
        "        schema=zone_schema\n",
        "    )\n",
        "\n",
        "    # 7. Print the schema of zone_df and show its first 5 rows\n",
        "    print(\"Schema of zone_df:\")\n",
        "    zone_df.printSchema()\n",
        "    print(\"\\nFirst 5 rows of zone_df:\")\n",
        "    zone_df.show(5, truncate=False)\n",
        "\n",
        "    # 8. Print the total count of rows in zone_df\n",
        "    print(f\"\\nTotal count of rows in zone_df: {zone_df.count()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed existing file: /content/taxi_zone_lookup_renamed.csv\n",
            "--2026-02-26 06:48:46--  https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
            "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 18.239.38.181, 18.239.38.147, 18.239.38.163, ...\n",
            "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|18.239.38.181|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12331 (12K) [text/csv]\n",
            "Saving to: ‘/content/taxi_zone_lookup.csv’\n",
            "\n",
            "\r          /content/   0%[                    ]       0  --.-KB/s               \r/content/taxi_zone_ 100%[===================>]  12.04K  --.-KB/s    in 0s      \n",
            "\n",
            "2026-02-26 06:48:46 (193 MB/s) - ‘/content/taxi_zone_lookup.csv’ saved [12331/12331]\n",
            "\n",
            "Downloaded file from https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv to: /content/taxi_zone_lookup.csv\n",
            "File renamed from /content/taxi_zone_lookup.csv to /content/taxi_zone_lookup_renamed.csv\n",
            "File /content/taxi_zone_lookup_renamed.csv successfully downloaded and verified (size: 12331 bytes).\n",
            "\n",
            "Schema of zone_df:\n",
            "root\n",
            " |-- LocationID: integer (nullable = true)\n",
            " |-- Borough: string (nullable = true)\n",
            " |-- Zone: string (nullable = true)\n",
            " |-- service_zone: string (nullable = true)\n",
            "\n",
            "\n",
            "First 5 rows of zone_df:\n",
            "+----------+-------------+-----------------------+------------+\n",
            "|LocationID|Borough      |Zone                   |service_zone|\n",
            "+----------+-------------+-----------------------+------------+\n",
            "|1         |EWR          |Newark Airport         |EWR         |\n",
            "|2         |Queens       |Jamaica Bay            |Boro Zone   |\n",
            "|3         |Bronx        |Allerton/Pelham Gardens|Boro Zone   |\n",
            "|4         |Manhattan    |Alphabet City          |Yellow Zone |\n",
            "|5         |Staten Island|Arden Heights          |Boro Zone   |\n",
            "+----------+-------------+-----------------------+------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Total count of rows in zone_df: 265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b179ecc",
        "outputId": "66439c54-fcc0-4680-ee11-4eb7ba3e93de"
      },
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Rename 'LocationID' in zone_df to 'PULocationID' to match clean_df\n",
        "zone_df_renamed = zone_df.withColumnRenamed(\"LocationID\", \"PULocationID\")\n",
        "\n",
        "# Perform a left join operation between clean_df and the renamed zone_df\n",
        "clean_df_with_borough = clean_df.join(\n",
        "    zone_df_renamed,\n",
        "    on=\"PULocationID\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Display the schema of the new clean_df_with_borough DataFrame\n",
        "print(\"Schema of clean_df_with_borough:\")\n",
        "clean_df_with_borough.printSchema()\n",
        "\n",
        "# Show the first 5 rows of the clean_df_with_borough DataFrame to verify the join\n",
        "print(\"\\nFirst 5 rows of clean_df_with_borough:\")\n",
        "clean_df_with_borough.show(5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema of clean_df_with_borough:\n",
            "root\n",
            " |-- PULocationID: integer (nullable = true)\n",
            " |-- _c0: integer (nullable = true)\n",
            " |-- VendorID: integer (nullable = true)\n",
            " |-- tpep_pickup_datetime: string (nullable = true)\n",
            " |-- tpep_dropoff_datetime: string (nullable = true)\n",
            " |-- passenger_count: integer (nullable = true)\n",
            " |-- trip_distance: double (nullable = true)\n",
            " |-- RatecodeID: integer (nullable = true)\n",
            " |-- store_and_fwd_flag: string (nullable = true)\n",
            " |-- DOLocationID: integer (nullable = true)\n",
            " |-- payment_type: integer (nullable = true)\n",
            " |-- fare_amount: double (nullable = true)\n",
            " |-- extra: double (nullable = true)\n",
            " |-- mta_tax: double (nullable = true)\n",
            " |-- tip_amount: double (nullable = true)\n",
            " |-- tolls_amount: double (nullable = true)\n",
            " |-- improvement_surcharge: double (nullable = true)\n",
            " |-- total_amount: double (nullable = true)\n",
            " |-- revenue_per_km: double (nullable = true)\n",
            " |-- pickup_time: timestamp (nullable = true)\n",
            " |-- pickup_hour: integer (nullable = true)\n",
            " |-- revenue_per_mile: double (nullable = true)\n",
            " |-- pickup_weekday: integer (nullable = true)\n",
            " |-- dropoff_time: timestamp (nullable = true)\n",
            " |-- trip_duration_minutes: double (nullable = true)\n",
            " |-- Borough: string (nullable = true)\n",
            " |-- Zone: string (nullable = true)\n",
            " |-- service_zone: string (nullable = true)\n",
            "\n",
            "\n",
            "First 5 rows of clean_df_with_borough:\n",
            "+------------+---------+--------+----------------------+----------------------+---------------+-------------+----------+------------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+------------------+-------------------+-----------+------------------+--------------+-------------------+---------------------+---------+----------------------------+------------+\n",
            "|PULocationID|_c0      |VendorID|tpep_pickup_datetime  |tpep_dropoff_datetime |passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|revenue_per_km    |pickup_time        |pickup_hour|revenue_per_mile  |pickup_weekday|dropoff_time       |trip_duration_minutes|Borough  |Zone                        |service_zone|\n",
            "+------------+---------+--------+----------------------+----------------------+---------------+-------------+----------+------------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+------------------+-------------------+-----------+------------------+--------------+-------------------+---------------------+---------+----------------------------+------------+\n",
            "|100         |24870114 |2       |03/25/2017 8:55:43 AM |03/25/2017 9:09:47 AM |6              |3.34         |1         |N                 |231         |1           |13.0       |0.0  |0.5    |2.76      |0.0         |0.3                  |16.56       |3.0988023952095802|2017-03-25 08:55:43|8          |4.958083832335329 |7             |2017-03-25 09:09:47|14.066666666666666   |Manhattan|Garment District            |Yellow Zone |\n",
            "|186         |35634249 |1       |04/11/2017 2:53:28 PM |04/11/2017 3:19:58 PM |1              |1.8          |1         |N                 |43          |1           |16.0       |0.0  |0.5    |4.0       |0.0         |0.3                  |20.8        |7.222222222222221 |2017-04-11 14:53:28|14         |11.555555555555555|3             |2017-04-11 15:19:58|26.5                 |Manhattan|Penn Station/Madison Sq West|Yellow Zone |\n",
            "|262         |106203690|1       |12/15/2017 7:26:56 AM |12/15/2017 7:34:08 AM |1              |1.0          |1         |N                 |236         |1           |6.5        |0.0  |0.5    |1.45      |0.0         |0.3                  |8.75        |5.46875           |2017-12-15 07:26:56|7          |8.75              |6             |2017-12-15 07:34:08|7.2                  |Manhattan|Yorkville East              |Yellow Zone |\n",
            "|188         |38942136 |2       |05/07/2017 1:17:59 PM |05/07/2017 1:48:14 PM |1              |3.7          |1         |N                 |97          |1           |20.5       |0.0  |0.5    |6.39      |0.0         |0.3                  |27.69       |4.677364864864864 |2017-05-07 13:17:59|13         |7.4837837837837835|1             |2017-05-07 13:48:14|30.25                |Brooklyn |Prospect-Lefferts Gardens   |Boro Zone   |\n",
            "|4           |30841670 |2       |04/15/2017 11:32:20 PM|04/15/2017 11:49:03 PM|1              |4.37         |1         |N                 |112         |2           |16.5       |0.5  |0.5    |0.0       |0.0         |0.3                  |17.8        |2.545766590389016 |2017-04-15 23:32:20|23         |4.073226544622425 |7             |2017-04-15 23:49:03|16.716666666666665   |Manhattan|Alphabet City               |Yellow Zone |\n",
            "+------------+---------+--------+----------------------+----------------------+---------------+-------------+----------+------------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+------------------+-------------------+-----------+------------------+--------------+-------------------+---------------------+---------+----------------------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "763ca128"
      },
      "source": [
        "## Compute Total Revenue per Borough\n",
        "\n",
        "### Subtask:\n",
        "Group the joined DataFrame by 'Borough' and calculate the sum of 'total_amount' to determine the total revenue for each Borough.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fc1b9f9",
        "outputId": "cd57742f-9408-4406-e490-13ee45709560"
      },
      "source": [
        "from pyspark.sql.functions import sum, col\n",
        "\n",
        "# Group by 'Borough' and calculate the sum of 'total_amount'\n",
        "total_revenue_per_borough = clean_df_with_borough.groupBy(\"Borough\") \\\n",
        "    .agg(sum(\"total_amount\").alias(\"total_revenue\")) \\\n",
        "    .orderBy(col(\"total_revenue\").desc())\n",
        "\n",
        "# Display the results\n",
        "print(\"Total Revenue per Borough:\")\n",
        "total_revenue_per_borough.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Revenue per Borough:\n",
            "+-------------+------------------+\n",
            "|Borough      |total_revenue     |\n",
            "+-------------+------------------+\n",
            "|Manhattan    |293290.699999942  |\n",
            "|Queens       |61479.07000000016 |\n",
            "|Unknown      |5578.510000000022 |\n",
            "|Brooklyn     |5308.000000000016 |\n",
            "|Bronx        |258.93000000000006|\n",
            "|N/A          |257.19            |\n",
            "|EWR          |152.3             |\n",
            "|Staten Island|7.3               |\n",
            "+-------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1f4f5f9"
      },
      "source": [
        "## Compute Average Trip Distance per Borough\n",
        "\n",
        "### Subtask:\n",
        "Group the joined DataFrame by 'Borough' and calculate the average of 'trip_distance' to find the average trip distance for each Borough.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9cddf02",
        "outputId": "c44dcc7f-e51a-4f65-aa23-b9dfd01c678a"
      },
      "source": [
        "from pyspark.sql.functions import avg, col\n",
        "\n",
        "# Group by 'Borough' and calculate the average of 'trip_distance'\n",
        "average_trip_distance_per_borough = clean_df_with_borough.groupBy(\"Borough\") \\\n",
        "    .agg(avg(\"trip_distance\").alias(\"average_trip_distance\")) \\\n",
        "    .orderBy(\"Borough\")\n",
        "\n",
        "# Display the results\n",
        "print(\"Average Trip Distance per Borough:\")\n",
        "average_trip_distance_per_borough.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Trip Distance per Borough:\n",
            "+-------------+---------------------+\n",
            "|Borough      |average_trip_distance|\n",
            "+-------------+---------------------+\n",
            "|Bronx        |3.8593750000000004   |\n",
            "|Brooklyn     |3.4224358974358995   |\n",
            "|EWR          |7.3                  |\n",
            "|Manhattan    |2.3759209050568213   |\n",
            "|N/A          |4.227142857142857    |\n",
            "|Queens       |11.148119970738836   |\n",
            "|Staten Island|1.43                 |\n",
            "|Unknown      |3.031472392638033    |\n",
            "+-------------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e383a253"
      },
      "source": [
        "## Display Borough Performance Table\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4a47d6c",
        "outputId": "4647187a-396d-462b-b1dd-3c84af3474d5"
      },
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Join the total_revenue_per_borough DataFrame with the average_trip_distance_per_borough DataFrame\n",
        "borough_performance_table = total_revenue_per_borough.join(\n",
        "    average_trip_distance_per_borough,\n",
        "    on=\"Borough\",\n",
        "    how=\"inner\"\n",
        ").orderBy(\"Borough\")\n",
        "\n",
        "# Display the borough_performance_table DataFrame\n",
        "print(\"Borough Performance Table:\")\n",
        "borough_performance_table.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Borough Performance Table:\n",
            "+-------------+------------------+---------------------+\n",
            "|Borough      |total_revenue     |average_trip_distance|\n",
            "+-------------+------------------+---------------------+\n",
            "|Bronx        |258.93000000000006|3.8593750000000004   |\n",
            "|Brooklyn     |5308.000000000016 |3.4224358974358995   |\n",
            "|EWR          |152.3             |7.3                  |\n",
            "|Manhattan    |293290.699999942  |2.3759209050568213   |\n",
            "|N/A          |257.19            |4.227142857142857    |\n",
            "|Queens       |61479.07000000016 |11.148119970738836   |\n",
            "|Staten Island|7.3               |1.43                 |\n",
            "|Unknown      |5578.510000000022 |3.031472392638033    |\n",
            "+-------------+------------------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a30b29b6"
      },
      "source": [
        "## Top 10 Pickup Zones by Trips\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b77cbfe2",
        "outputId": "896afaf4-14e7-45de-bdc3-d46c51a93338"
      },
      "source": [
        "from pyspark.sql.functions import col, count\n",
        "\n",
        "# Group by 'Zone', count trips, and order the results\n",
        "top_10_pickup_zones = clean_df_with_borough.groupBy(\"Zone\") \\\n",
        "    .agg(count(\"*\").alias(\"total_pickups\")) \\\n",
        "    .orderBy(col(\"total_pickups\").desc()) \\\n",
        "    .limit(10)\n",
        "\n",
        "# Display the results\n",
        "print(\"Top 10 Pickup Zones by Trips:\")\n",
        "top_10_pickup_zones.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Pickup Zones by Trips:\n",
            "+----------------------------+-------------+\n",
            "|Zone                        |total_pickups|\n",
            "+----------------------------+-------------+\n",
            "|Upper East Side South       |888          |\n",
            "|Midtown Center              |858          |\n",
            "|Penn Station/Madison Sq West|786          |\n",
            "|Upper East Side North       |784          |\n",
            "|Midtown East                |771          |\n",
            "|Union Sq                    |746          |\n",
            "|Murray Hill                 |745          |\n",
            "|Clinton East                |740          |\n",
            "|Times Sq/Theatre District   |735          |\n",
            "|Lincoln Square East         |647          |\n",
            "+----------------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "814b5db0"
      },
      "source": [
        "## Top 10 Dropoff Zones by Revenue\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "928d1dab"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed due to an `IndentationError`. I need to correct the indentation for the `.limit(10)` method call to properly chain it with the `.orderBy()` method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80be6445"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code consistently failed due to an `IndentationError` where `.limit(10)` was on a new, incorrectly indented line. I will correct this by placing `.limit(10)` on the same line as `.orderBy(col(\"total_revenue\").desc())` to ensure proper method chaining.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "767ccdf3",
        "outputId": "7957f740-8446-4477-ee63-e785217c2e64"
      },
      "source": [
        "from pyspark.sql.functions import sum, col\n",
        "\n",
        "# 1. Group by 'DOLocationID' and calculate the sum of 'total_amount'\n",
        "dropoff_revenue_df = clean_df_with_borough.groupBy(\"DOLocationID\") \\\n",
        "    .agg(sum(\"total_amount\").alias(\"total_revenue\"))\n",
        "\n",
        "# 2. Join with zone_df_renamed to get the dropoff zone names\n",
        "top_10_dropoff_zones = dropoff_revenue_df.join(\n",
        "    zone_df_renamed.withColumnRenamed(\"PULocationID\", \"DOLocationID\"),\n",
        "    on=\"DOLocationID\",\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "# 3. Order the resulting DataFrame by 'total_revenue' in descending order\n",
        "# 4. Select the top 10 rows from the ordered DataFrame\n",
        "top_10_dropoff_zones = top_10_dropoff_zones.orderBy(col(\"total_revenue\").desc()).limit(10)\n",
        "\n",
        "# 5. Display the results\n",
        "print(\"Top 10 Dropoff Zones by Revenue:\")\n",
        "top_10_dropoff_zones.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Dropoff Zones by Revenue:\n",
            "+------------+------------------+---------+----------------------------+------------+\n",
            "|DOLocationID|total_revenue     |Borough  |Zone                        |service_zone|\n",
            "+------------+------------------+---------+----------------------------+------------+\n",
            "|230         |12676.199999999915|Manhattan|Times Sq/Theatre District   |Yellow Zone |\n",
            "|132         |12447.010000000007|Queens   |JFK Airport                 |Airports    |\n",
            "|161         |12306.129999999943|Manhattan|Midtown Center              |Yellow Zone |\n",
            "|138         |10753.03          |Queens   |LaGuardia Airport           |Airports    |\n",
            "|162         |10112.499999999993|Manhattan|Midtown East                |Yellow Zone |\n",
            "|236         |9514.820000000014 |Manhattan|Upper East Side North       |Yellow Zone |\n",
            "|48          |9286.47000000001  |Manhattan|Clinton East                |Yellow Zone |\n",
            "|170         |9098.60000000003  |Manhattan|Murray Hill                 |Yellow Zone |\n",
            "|186         |8673.650000000023 |Manhattan|Penn Station/Madison Sq West|Yellow Zone |\n",
            "|237         |8528.280000000033 |Manhattan|Upper East Side South       |Yellow Zone |\n",
            "+------------+------------------+---------+----------------------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40f102b7",
        "outputId": "4a5515da-3b39-414c-a433-b5f903065917"
      },
      "source": [
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "# Add the 'distance_category' column\n",
        "clean_df_with_borough = clean_df_with_borough.withColumn(\n",
        "    \"distance_category\",\n",
        "    when(col(\"trip_distance\") < 1, \"Micro\")\n",
        "    .when((col(\"trip_distance\") >= 1) & (col(\"trip_distance\") < 3), \"Short\")\n",
        "    .when((col(\"trip_distance\") >= 3) & (col(\"trip_distance\") < 10), \"Medium\")\n",
        "    .otherwise(\"Long\")\n",
        ")\n",
        "\n",
        "print(\"Added 'distance_category' column to clean_df_with_borough.\")\n",
        "clean_df_with_borough.select(\"trip_distance\", \"distance_category\").show(5)\n",
        "clean_df_with_borough.groupBy(\"distance_category\").count().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 'distance_category' column to clean_df_with_borough.\n",
            "+-------------+-----------------+\n",
            "|trip_distance|distance_category|\n",
            "+-------------+-----------------+\n",
            "|         3.34|           Medium|\n",
            "|          1.8|            Short|\n",
            "|          1.0|            Short|\n",
            "|          3.7|           Medium|\n",
            "|         4.37|           Medium|\n",
            "+-------------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----------------+-----+\n",
            "|distance_category|count|\n",
            "+-----------------+-----+\n",
            "|           Medium| 4544|\n",
            "|            Micro| 5526|\n",
            "|             Long| 1312|\n",
            "|            Short|11155|\n",
            "+-----------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adc6cc82",
        "outputId": "6ae42c1e-ee34-4c47-916a-58230f7019af"
      },
      "source": [
        "from pyspark.sql.functions import sum, avg, count\n",
        "\n",
        "# Group by 'distance_category' and calculate the required metrics\n",
        "distance_category_performance = clean_df_with_borough.groupBy(\"distance_category\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"total_trips\"),\n",
        "        avg(\"total_amount\").alias(\"average_total_revenue\"),\n",
        "        avg(\"trip_duration_minutes\").alias(\"average_trip_duration_minutes\")\n",
        "    ) \\\n",
        "    .orderBy(\"distance_category\")\n",
        "\n",
        "# Display the results\n",
        "print(\"Distance Category Performance Table:\")\n",
        "distance_category_performance.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance Category Performance Table:\n",
            "+-----------------+-----------+---------------------+-----------------------------+\n",
            "|distance_category|total_trips|average_total_revenue|average_trip_duration_minutes|\n",
            "+-----------------+-----------+---------------------+-----------------------------+\n",
            "|Long             |1312       |57.0151753048781     |44.744258130081285           |\n",
            "|Medium           |4544       |24.5552310739446     |27.5708993544601             |\n",
            "|Micro            |5526       |7.686825913861883    |7.761693207865856            |\n",
            "|Short            |11155      |12.323775885254      |14.136621843717345           |\n",
            "+-----------------+-----------+---------------------+-----------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7687626c"
      },
      "source": [
        "### Task 9: Payment Behavior Insights\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c92e42b",
        "outputId": "7a654eac-ea0d-4943-9b7d-1cce73aef477"
      },
      "source": [
        "from pyspark.sql.functions import sum, avg, count, col, lit\n",
        "\n",
        "# Calculate total trips for percentage calculation\n",
        "total_trips_all_types = clean_df_with_borough.count()\n",
        "\n",
        "# Group by 'payment_type' and calculate the required metrics\n",
        "payment_behavior = clean_df_with_borough.groupBy(\"payment_type\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"total_trips\"),\n",
        "        avg(\"total_amount\").alias(\"average_revenue\"),\n",
        "        sum(\"total_amount\").alias(\"total_revenue\")\n",
        "    ) \\\n",
        "    .withColumn(\"percentage_contribution\", (col(\"total_trips\") / lit(total_trips_all_types)) * 100) \\\n",
        "    .orderBy(\"payment_type\")\n",
        "\n",
        "# Display the results\n",
        "print(\"Payment Analytics Summary:\")\n",
        "payment_behavior.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Payment Analytics Summary:\n",
            "+------------+-----------+------------------+------------------+-----------------------+\n",
            "|payment_type|total_trips|average_revenue   |total_revenue     |percentage_contribution|\n",
            "+------------+-----------+------------------+------------------+-----------------------+\n",
            "|1           |15203      |17.555421298426573|266895.06999997917|67.4579580245818       |\n",
            "|2           |7203       |13.508571428573047|97302.24000001166 |31.960775613435683     |\n",
            "|3           |94         |15.858829787234018|1490.7299999999977|0.41709189333096686    |\n",
            "|4           |37         |17.404324324324328|643.9600000000002 |0.16417446865155078    |\n",
            "+------------+-----------+------------------+------------------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9b493a6"
      },
      "source": [
        "### Task 10: Final Operational Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b09de8dd",
        "outputId": "c8099326-8c75-46fd-b8a0-314873dceae1"
      },
      "source": [
        "from pyspark.sql.functions import sum, avg, count, col\n",
        "\n",
        "# Group by 'Borough' and calculate the required aggregate metrics\n",
        "operational_dataset = clean_df_with_borough.groupBy(\"Borough\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"Total_Trips\"),\n",
        "        sum(\"total_amount\").alias(\"Total_Revenue\"),\n",
        "        avg(\"total_amount\").alias(\"Average_Revenue_per_Trip\"),\n",
        "        avg(\"trip_distance\").alias(\"Average_Distance\"),\n",
        "        avg(\"trip_duration_minutes\").alias(\"Average_Duration\")\n",
        "    ) \\\n",
        "    .orderBy(col(\"Total_Revenue\").desc())\n",
        "\n",
        "print(\"Summarized operational dataset created:\")\n",
        "operational_dataset.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarized operational dataset created:\n",
            "+-------------+-----------+------------------+------------------------+------------------+------------------+\n",
            "|Borough      |Total_Trips|Total_Revenue     |Average_Revenue_per_Trip|Average_Distance  |Average_Duration  |\n",
            "+-------------+-----------+------------------+------------------------+------------------+------------------+\n",
            "|Manhattan    |20507      |293290.699999942  |14.30197981176876       |2.3759209050568213|15.86567188439703 |\n",
            "|Queens       |1367       |61479.07000000016 |44.973716166788705      |11.148119970738836|34.223945379175774|\n",
            "|Unknown      |326        |5578.510000000022 |17.111993865030744      |3.031472392638033 |23.62888548057258 |\n",
            "|Brooklyn     |312        |5308.000000000016 |17.012820512820564      |3.4224358974358995|14.212446581196579|\n",
            "|Bronx        |16         |258.93000000000006|16.183125000000004      |3.8593750000000004|12.159375         |\n",
            "|N/A          |7          |257.19            |36.74142857142857       |4.227142857142857 |12.685714285714285|\n",
            "|EWR          |1          |152.3             |152.3                   |7.3               |0.6833333333333333|\n",
            "|Staten Island|1          |7.3               |7.3                     |1.43              |8.966666666666667 |\n",
            "+-------------+-----------+------------------+------------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe55bb6c",
        "outputId": "58b0f2c5-4d15-4848-8ac7-2297278bfef5"
      },
      "source": [
        "# Define the output path for the Parquet file\n",
        "output_path = \"/content/operational_dataset.parquet\"\n",
        "\n",
        "# Store the result in Parquet format\n",
        "operational_dataset.write.mode(\"overwrite\").parquet(output_path)\n",
        "\n",
        "print(f\"Operational dataset saved to {output_path} in Parquet format.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Operational dataset saved to /content/operational_dataset.parquet in Parquet format.\n"
          ]
        }
      ]
    }
  ]
}